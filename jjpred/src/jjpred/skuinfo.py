"""Functions for generating a SKU information dataframe which contains:

* ``is_active``: whether a SKU is active
* :py:const:`ALL_SKU_IDS` + :py:const:`SEASON_IDS` + :py:const:`PAUSE_PLAN_IDS`
  + :py:const:`NOVELTY_FLAGS`

This dataframe is initialized by the dispatcher, and then extended further with
various information generated by the prediction and dispatching processes. A
full list of all the objects generated is given in :py:mod:`datagroups`.
"""

from __future__ import annotations

from collections import defaultdict
import sys
from typing import Literal
import polars as pl

from jjpred.analysisdefn import AnalysisDefn, RefillDefn
from jjpred.channel import Channel
from jjpred.datagroups import (
    ALL_SKU_AND_CHANNEL_IDS,
    ALL_SKU_IDS,
    CHANNEL_IDS,
    PAUSE_PLAN_IDS,
    SEASON_IDS,
    WHOLE_SKU_IDS,
)
from jjpred.countryflags import CountryFlags
from jjpred.database import DataBase
from jjpred.readsheet import DataVariant
from jjpred.readsupport.marketing import ConfigData
from jjpred.sku import Sku
from jjpred.structlike import MemberType
from jjpred.utils.fileio import read_meta_info
from jjpred.utils.polars import (
    FilterStructs,
    OverrideLeft,
    binary_partition_strict,
    find_dupes,
    join_and_coalesce,
    struct_filter,
)
from jjpred.utils.typ import PolarsLit, ScalarOrList, normalize_as_list


def get_all_sku_currentness_info(
    analysis_id_or_database: RefillDefn | DataBase,
) -> pl.DataFrame:
    if isinstance(analysis_id_or_database, RefillDefn):
        analysis_defn = analysis_id_or_database
        dispatch_date = analysis_defn.dispatch_date
    elif isinstance(analysis_id_or_database, DataBase):
        database = analysis_id_or_database
        analysis_defn = database.analysis_defn
        dispatch_date = database.dispatch_date()
    else:
        raise ValueError(f"No logic to handle {analysis_id_or_database=}")

    current_year = dispatch_date.year - 2000
    next_year = (dispatch_date.year + 1) - 2000

    all_sku_info = (
        read_meta_info(analysis_defn, "all_sku")
        .select(ALL_SKU_IDS + SEASON_IDS + PAUSE_PLAN_IDS + ["status"])
        .with_columns(
            pl.col("status").eq("active").alias("is_active").fill_null(False)
        )
        .drop("status")
        .with_columns(
            sku_latest_year=pl.col("sku_latest_year").fill_null(-1),
        )
        .with_columns(
            is_current_print=pl.col("sku_year_history")
            .list.eval(pl.element().eq(current_year))
            .list.any()
            .alias("is_current_print")
            .fill_null(False)
        )
        .with_columns(
            is_new_category=pl.col("category_year_history")
            .list.eval(pl.element().is_in([current_year, next_year]))
            .list.all()
            .fill_null(False)
        )
        .with_columns(
            is_new_sku=(
                pl.col("is_current_print")
                & pl.col("sku_year_history")
                .list.eval(pl.element().is_in([current_year, next_year]))
                .list.all()
                .fill_null(False)
            )
        )
        .with_columns(
            is_next_year_print=(
                pl.col("sku_latest_year").eq(next_year)
                & ~pl.col("is_current_print")
            ).fill_null(False)
        )
    )

    find_dupes(all_sku_info, ALL_SKU_IDS, raise_error=True)

    return all_sku_info


def override_sku_info(
    all_sku_info: pl.DataFrame,
    extension_df: pl.DataFrame,
    fill_null_value: PolarsLit
    | defaultdict[str, PolarsLit | None]
    | None = None,
    create_info_columns: ScalarOrList[str | pl.Expr] | None = None,
    create_missing_info_flags: bool = False,
    join_nulls: bool = True,
    dupe_check_index: list[str] | None = None,
) -> pl.DataFrame:
    if len(extension_df) == 0:
        return all_sku_info

    existing_id_columns = [
        x
        for x in all_sku_info.columns
        if (
            x
            in ["a_sku"]
            + Sku.members(MemberType.META)
            + Channel.members()
            + [
                "pause_plan",
                "season",
                "sku_year_history",
                "category_year_history",
                "sku_latest_year",
            ]
        )
    ]

    existing_data_columns = [
        x for x in all_sku_info.columns if x not in existing_id_columns
    ]
    shared_id_columns = [
        x for x in extension_df.columns if x in existing_id_columns
    ]

    data_columns = [
        x
        for x in extension_df.columns
        if ((x not in shared_id_columns) and (x not in existing_data_columns))
    ]

    if dupe_check_index is None:
        dupe_check_index = [
            x
            for x in all_sku_info.columns
            if x
            in (["a_sku"] + Sku.members(MemberType.META) + Channel.members())
        ]

    find_dupes(all_sku_info, dupe_check_index, raise_error=True)

    all_sku_info = join_and_coalesce(
        all_sku_info,
        extension_df,
        OverrideLeft(existing_id_columns),
        join_nulls=join_nulls,
        dupe_check_index=dupe_check_index,
    )

    create_info_columns = normalize_as_list(create_info_columns)

    string_entries = [x for x in create_info_columns if isinstance(x, str)]
    if create_missing_info_flags:
        unaccounted_data_columns = [
            x for x in data_columns if x not in string_entries
        ]
        string_entries = string_entries + unaccounted_data_columns

    expr_entries = [x for x in create_info_columns if isinstance(x, pl.Expr)]

    assert all([x in data_columns for x in string_entries]), [
        x for x in string_entries if x not in data_columns
    ]
    final_create_columns = expr_entries + [
        pl.col(x).is_null().alias(f"no_{x}_info") for x in string_entries
    ]

    if len(final_create_columns) > 0:
        all_sku_info = all_sku_info.with_columns(
            x for x in final_create_columns
        )

    if fill_null_value is not None:
        if isinstance(fill_null_value, PolarsLit):
            # https://stackoverflow.com/a/10452819
            fill_null_value = defaultdict(lambda x=fill_null_value: x, {})

        assert isinstance(fill_null_value, defaultdict), type(fill_null_value)

        wrong_fill_type: list[tuple[str, pl.DataType, PolarsLit]] = []
        warn_fill_type: list[tuple[str, pl.DataType, PolarsLit]] = []

        for x in data_columns:
            fill_value = fill_null_value[x]
            required_type = all_sku_info[x].dtype

            if fill_value is not None and fill_value.dtype != required_type:
                if not (
                    required_type.is_numeric()
                    and fill_value.dtype.is_numeric()
                ):
                    wrong_fill_type.append((x, required_type, fill_value))
                else:
                    warn_fill_type.append((x, required_type, fill_value))

        if len(wrong_fill_type) > 0:
            raise ValueError(
                f"The following data columns were not given fill values of the right type: {wrong_fill_type}"
            )
        if len(warn_fill_type) > 0:
            print(f"{warn_fill_type=}")

        if len(fill_null_value) > 0:
            all_sku_info = all_sku_info.with_columns(
                pl.col(x).fill_null(v.lit)
                for x, v in fill_null_value.items()
                if v is not None
            )

    return all_sku_info


def attach_refill_info_from_config(
    all_sku_info: pl.DataFrame, config_data: ConfigData
) -> pl.DataFrame:
    all_sku_info = override_sku_info(
        all_sku_info,
        config_data.no_refill.with_columns(
            is_config_paused=pl.col("refill_request").eq(-1)
        ).drop("refill_request"),
        fill_null_value=PolarsLit(False),
    )

    all_sku_info = override_sku_info(
        all_sku_info, config_data.refill, fill_null_value=PolarsLit(0)
    )

    return all_sku_info


def attach_channel_info(
    all_sku_info: pl.DataFrame, channel_info: pl.DataFrame
) -> pl.DataFrame:
    return all_sku_info.join(channel_info, how="cross", on=None).with_columns(
        is_master_paused=pl.col("country_flag")
        .and_(pl.col("pause_plan"))
        .gt(0)
    )


def calculate_wh_dispatchable(wh_stock: pl.DataFrame) -> pl.DataFrame:
    return wh_stock.with_columns(
        pl.when(
            pl.col("wh_stock").gt(0)
            & pl.col("wh_stock").ge(pl.col("min_keep"))
        )
        .then(
            pl.col("wh_stock").sub(pl.col("min_keep")).alias("wh_dispatchable")
        )
        .otherwise(pl.lit(0, dtype=pl.Int64()).alias("wh_dispatchable"))
    )


def attach_inventory_info(
    db: DataBase,
    config_data: ConfigData,
    all_sku_info: pl.DataFrame,
    warehouse_filter: pl.Expr,
    warehouse_min_keep_qty: int,
    filters: FilterStructs | None = None,
) -> pl.DataFrame:
    inv_df = struct_filter(db.dfs[DataVariant.Inventory], filters)
    wh_stock, ch_stock = binary_partition_strict(
        inv_df.select(WHOLE_SKU_IDS + CHANNEL_IDS + ["stock"]),
        warehouse_filter,
    )
    wh_stock = (
        wh_stock.filter(
            pl.col("country_flag").eq(int(CountryFlags.CA)),
        )
        .drop(Channel.members())
        .join(
            config_data.min_keep,
            on=WHOLE_SKU_IDS,
            how="left",
            validate="m:1",
            join_nulls=True,
        )
        .with_columns(
            min_keep_default=pl.lit(warehouse_min_keep_qty, pl.Int64()),
        )
        .with_columns(
            min_keep=pl.max_horizontal("min_keep", "min_keep_default")
        )
        .drop("min_keep_default")
        .rename({"stock": "wh_stock"})
    )
    ch_stock = ch_stock.rename({"stock": "ch_stock"})

    wh_info = calculate_wh_dispatchable(wh_stock).select(
        WHOLE_SKU_IDS + ["wh_stock", "min_keep", "wh_dispatchable"]
    )

    all_sku_info = override_sku_info(
        all_sku_info,
        wh_info,
        fill_null_value=PolarsLit(0),
        create_info_columns=[
            "wh_stock",
            (
                pl.col("wh_stock").is_not_null() & pl.col("wh_stock").lt(0)
            ).alias("negative_wh_stock"),
            (
                pl.col("wh_stock").is_null() | pl.col("wh_dispatchable").eq(0)
            ).alias("zero_wh_dispatchable"),
        ],
        dupe_check_index=ALL_SKU_AND_CHANNEL_IDS,
    )

    all_sku_info = override_sku_info(
        all_sku_info,
        ch_stock,
        fill_null_value=PolarsLit(0),
        create_info_columns=[
            "ch_stock",
            (
                (
                    ~(
                        pl.col("ch_stock").is_not_null()
                        & pl.col("ch_stock").gt(0)
                    )
                ).alias("zero_ch_stock")
            ),
        ],
    )

    return all_sku_info


def fetch_master_sku_info(
    analysis_defn_or_db: AnalysisDefn | DataBase,
    info_type: Literal["active_sku", "all_sku"],
) -> pl.DataFrame:
    if isinstance(analysis_defn_or_db, AnalysisDefn):
        fetched_sku_info = read_meta_info(analysis_defn_or_db, info_type)
    else:
        fetched_sku_info = analysis_defn_or_db.meta_info.__getattribute__(
            info_type
        )
    return fetched_sku_info


def check_aggregated_sku_info_for_duplicates(
    aggregated_sku_info: pl.DataFrame, id_cols: list[str]
) -> pl.DataFrame:
    sku_info = aggregated_sku_info
    for y in [x for x in aggregated_sku_info.columns if x not in id_cols]:
        if isinstance(aggregated_sku_info[y].dtype, pl.List):
            dupes = aggregated_sku_info.filter(pl.col(y).list.len().gt(1))
            if len(dupes) > 0:
                print(f"{y}:")
                sys.displayhook(dupes)
                raise ValueError("Found dupes in SKU info!")

            sku_info = sku_info.with_columns(pl.col(y).list.first().alias(y))

    return aggregated_sku_info


def get_checked_a_sku_info(
    analysis_defn_or_db: AnalysisDefn | DataBase,
    only_active_sku: bool,
    info_columns: list[str] = ["season_history"],
    rename_brc_ylw_to_brc_yel: bool = True,
    rename_icp_to_ipc: bool = True,
) -> pl.DataFrame:
    if only_active_sku:
        info_type = "active_sku"
    else:
        info_type = "all_sku"

    sku_info = fetch_master_sku_info(analysis_defn_or_db, info_type)

    cat_print_size = Sku.members(MemberType.PRIMARY)
    info_columns = [
        x
        for x in info_columns
        if ((x in sku_info.columns) and (x not in cat_print_size))
    ]

    if rename_brc_ylw_to_brc_yel:
        sku_info = sku_info.with_columns(
            print=pl.when(pl.col.category.eq("BRC") & pl.col.print.eq("YEL"))
            .then(pl.lit("YLW", dtype=sku_info["print"].dtype))
            .otherwise(pl.col.print)
        )

    if rename_icp_to_ipc:
        sku_info = sku_info.with_columns(
            category=pl.when(pl.col.category.eq("ICP"))
            .then(pl.lit("IPC", dtype=sku_info["category"].dtype))
            .otherwise(pl.col.category)
        )

    sku_info = (
        sku_info.sort("sku_latest_year")
        .with_columns(
            rank=pl.col.sku_latest_year.rank("ordinal").over("a_sku")
        )
        .with_columns(max_rank=pl.col.rank.max().over("a_sku"))
        .filter(pl.col.rank.eq(pl.col.max_rank))
        .drop("max_rank")
        .select(
            ["a_sku"] + cat_print_size + info_columns,
        )
        .group_by("a_sku")
        .agg(pl.col(x).unique() for x in cat_print_size + info_columns)
    )

    if "season_history" in info_columns:
        sku_info = sku_info.with_columns(
            pl.col.season_history.list.sort(descending=True)
            .list.eval(pl.element().cast(pl.String()))
            .list.join(",")
        )
        sku_info = sku_info.cast(
            {
                "season_history": pl.Enum(
                    sku_info["season_history"].unique().sort()
                )
            }
        )

    for y in cat_print_size + info_columns:
        if isinstance(sku_info[y].dtype, pl.List):
            dupes = sku_info.filter(pl.col(y).list.len().gt(1))
            if len(dupes) > 0:
                print(f"{y}:")
                sys.displayhook(dupes)
                raise ValueError("Found dupes in SKU info!")

            sku_info = sku_info.with_columns(pl.col(y).list.first().alias(y))

    return sku_info


def get_checked_category_level_sku_info(
    db: DataBase,
    category_info_columns: list[str] = ["season", "category_year_history"],
) -> pl.DataFrame:
    extracted_info = get_checked_a_sku_info(
        db, False, info_columns=category_info_columns
    ).select(["category"] + category_info_columns)

    season_df = extracted_info.group_by("category").agg(
        pl.col(x).unique() for x in category_info_columns
    )

    # eliminating dupes
    if "category_year_history" in category_info_columns:
        season_df = season_df.with_columns(
            pl.col.category_year_history.list.eval(pl.element().explode())
            .list.unique()
            .list.sort(descending=True)
        )

    if "season_history" in category_info_columns:
        assert (
            len(season_df.filter(pl.col.season_history.list.len().gt(1))) == 0
        )
        season_df = season_df.with_columns(
            season=pl.col.season_history.list.first()
        )

    if "season" in category_info_columns:
        assert len(season_df.filter(pl.col.season.list.len().gt(1))) == 0
        season_df = season_df.with_columns(season=pl.col.season.list.first())

    find_dupes(season_df, ["category"], raise_error=True)

    return season_df
